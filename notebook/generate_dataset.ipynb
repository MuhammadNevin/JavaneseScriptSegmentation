{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import numpy as np\n",
    "import random, math\n",
    "import copy\n",
    "\n",
    "def read_json(fname, key_int=False):\n",
    "    with open(fname, 'r') as file:\n",
    "        data = file.read()\n",
    "        json_data = json.loads(data)\n",
    "        \n",
    "        if not key_int:\n",
    "            return json_data\n",
    "        \n",
    "        json_data = {int(key): value for key, value in json_data.items()}\n",
    "        return json_data\n",
    "    \n",
    "CHAR_INDICES = read_json('../models/CHAR_INDICES.json', key_int=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 13\n",
      "Train size: 9\n",
      "Val size: 2\n",
      "Test size: 2\n",
      "TRAIN FILES:\n",
      "['../sastra/1437.txt', '../sastra/1537.txt', '../sastra/1533.txt', '../sastra/1447.txt', '../sastra/1539.txt', '../sastra/1403.txt', '../sastra/1535.txt', '../sastra/1463.txt', '../sastra/1538.txt']\n",
      "VAL FILES:\n",
      "['../sastra/1260.txt', '../sastra/1536.txt']\n",
      "TEST FILES:\n",
      "['../sastra/1534.txt', '../sastra/1462.txt']\n",
      "Train total char count: 612172\n",
      "Val total char count: 149084\n",
      "Test total char count: 161806\n",
      "Train char percentage: 0.6631970550190561\n",
      "Val char percentage: 0.16151027774949028\n",
      "Test char percentage: 0.17529266723145356\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "file_list = glob.glob('../sastra/*.txt', recursive=True)\n",
    "print('Total files:', len(file_list))\n",
    "\n",
    "# get only 20 files\n",
    "file_list = file_list[:13]\n",
    "\n",
    "#train val test size\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "#splitting the data\n",
    "# random.shuffle(file_list)\n",
    "# train_list = file_list[:int(len(file_list)*train_size)]\n",
    "# val_list = file_list[int(len(file_list)*train_size):int(len(file_list)*(train_size+val_size))]\n",
    "# test_list = file_list[int(len(file_list)*(train_size+val_size)):]\n",
    "\n",
    "# train_list = ['../sastra/2682.txt', '../sastra/1704.txt', '../sastra/2565.txt', '../sastra/1537.txt', '../sastra/1903.txt', '../sastra/1260.txt', '../sastra/2732.txt', '../sastra/1910.txt', '../sastra/220.txt', '../sastra/2224.txt', '../sastra/2566.txt', '../sastra/1403.txt', '../sastra/1906.txt', '../sastra/1447.txt', '../sastra/2810.txt', '../sastra/224.txt', '../sastra/3030.txt', '../sastra/2225.txt', '../sastra/1909.txt', '../sastra/2571.txt', '../sastra/2685.txt', '../sastra/2568.txt', '../sastra/2812.txt', '../sastra/1900.txt', '../sastra/1908.txt', '../sastra/221.txt', '../sastra/1538.txt', '../sastra/2770.txt', '../sastra/2570.txt', '../sastra/2567.txt', '../sastra/1534.txt', '../sastra/2813.txt', '../sastra/1462.txt', '../sastra/2811.txt', '../sastra/1902.txt', '../sastra/2684.txt', '../sastra/1463.txt', '../sastra/2681.txt', '../sastra/2733.txt', '../sastra/3032.txt']\n",
    "# val_list = ['../sastra/2687.txt', '../sastra/2564.txt', '../sastra/3031.txt', '../sastra/1901.txt', '../sastra/2683.txt', '../sastra/1904.txt', '../sastra/1907.txt', '../sastra/219.txt', '../sastra/1657.txt']\n",
    "# test_list = ['../sastra/2686.txt', '../sastra/222.txt', '../sastra/1535.txt', '../sastra/2569.txt', '../sastra/1539.txt', '../sastra/1536.txt', '../sastra/1437.txt', '../sastra/1533.txt', '../sastra/1905.txt']\n",
    "\n",
    "# train_list = ['../sastra/1463.txt', '../sastra/1538.txt', '../sastra/1534.txt', '../sastra/1901.txt', '../sastra/1437.txt', '../sastra/1539.txt', '../sastra/1537.txt', '../sastra/1903.txt', '../sastra/1403.txt', '../sastra/1260.txt', '../sastra/1536.txt', '../sastra/1657.txt', '../sastra/1447.txt', '../sastra/1900.txt']\n",
    "# val_list = ['../sastra/1533.txt', '../sastra/1462.txt', '../sastra/1535.txt']\n",
    "# test_list = ['../sastra/1902.txt', '../sastra/1904.txt', '../sastra/1704.txt']\n",
    "\n",
    "train_list = ['../sastra/1437.txt', '../sastra/1537.txt', '../sastra/1533.txt', '../sastra/1447.txt', '../sastra/1539.txt', '../sastra/1403.txt', '../sastra/1535.txt', '../sastra/1463.txt', '../sastra/1538.txt']\n",
    "val_list =  ['../sastra/1260.txt', '../sastra/1536.txt']\n",
    "test_list =  ['../sastra/1534.txt', '../sastra/1462.txt']\n",
    "\n",
    "print('Train size:', len(train_list))\n",
    "print('Val size:', len(val_list))\n",
    "print('Test size:', len(test_list))\n",
    "\n",
    "print(\"TRAIN FILES:\")\n",
    "print(train_list)\n",
    "\n",
    "print(\"VAL FILES:\")\n",
    "print(val_list)\n",
    "\n",
    "print(\"TEST FILES:\")\n",
    "print(test_list)\n",
    "\n",
    "# compute total char count in each file\n",
    "def get_total_char_count(file_list):\n",
    "    total_char_count = 0\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            total_char_count += len(f.read())\n",
    "    return total_char_count\n",
    "\n",
    "train_total_char_count = get_total_char_count(train_list)\n",
    "val_total_char_count = get_total_char_count(val_list)\n",
    "test_total_char_count = get_total_char_count(test_list)\n",
    "\n",
    "print('Train total char count:', train_total_char_count)\n",
    "print('Val total char count:', val_total_char_count)\n",
    "print('Test total char count:', test_total_char_count)\n",
    "\n",
    "# train %\n",
    "train_char_percentage = train_total_char_count / (train_total_char_count + val_total_char_count + test_total_char_count)\n",
    "val_char_percentage = val_total_char_count / (train_total_char_count + val_total_char_count + test_total_char_count)\n",
    "test_char_percentage = test_total_char_count / (train_total_char_count + val_total_char_count + test_total_char_count)\n",
    "\n",
    "print('Train char percentage:', train_char_percentage)\n",
    "print('Val char percentage:', val_char_percentage)\n",
    "print('Test char percentage:', test_char_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DICTIONARY\n",
    "\n",
    "dict_path = \"../models/dictionary.txt\"\n",
    "def create_dict(file_list):\n",
    "    words = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            delimeter = \"|\"\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(delimeter)\n",
    "                for word in line:\n",
    "                    if word not in words:\n",
    "                        words.append(word)\n",
    "\n",
    "\n",
    "    with open(dict_path, 'w') as f:\n",
    "        for word in words:\n",
    "            f.write(word + '\\n')\n",
    "\n",
    "    return words\n",
    "\n",
    "def read_dict():\n",
    "    with open(dict_path, 'r') as f:\n",
    "        words = f.read().splitlines()\n",
    "    return words\n",
    "\n",
    "# create dictionary\n",
    "file_list = train_list + val_list\n",
    "create_dict(file_list)\n",
    "\n",
    "DICT = read_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def replace(text, wordlist):\n",
    "    for new_word, list_ in wordlist.items():\n",
    "        for old_word in list_:\n",
    "            text = text.replace(old_word.lower(), new_word)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocessing(text, wordlist):\n",
    "    text = text.lower()  # to lower-case\n",
    "    text = replace(text, wordlist)\n",
    "\n",
    "    # check if char is in CHAR_INDICES\n",
    "    for char in text:\n",
    "        if char not in CHAR_INDICES:\n",
    "            text = text.replace(char, '')\n",
    "            \n",
    "    return text\n",
    "\n",
    "def generate_dict_vector(text):\n",
    "    vect = [0.0] * len(text)\n",
    "    for i in range(len(text)):\n",
    "        current_text = text[:i+1]\n",
    "        if current_text in DICT:\n",
    "            vect[i] = 1.0\n",
    "        else:\n",
    "            vect[i] = 0.0\n",
    "\n",
    "    return vect\n",
    "\n",
    "def create_dataset(text, look_back, look_front, file_name, type_):\n",
    "    X, y = [], []\n",
    "    X_ngram = []\n",
    "    text = '|' + text\n",
    "    data = ['?'] * look_back\n",
    "\n",
    "    directory = f\"../generated/{type_}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    dataset_path = f\"../generated/{type_}/{file_name}.txt\"\n",
    "    progress_bar = tqdm(total=len(text), desc=\"Processing\")\n",
    "\n",
    "    for i in range(1, len(text)):\n",
    "        current_char = text[i]\n",
    "        before_char = text[i-1]\n",
    "\n",
    "        if current_char == '|':\n",
    "            continue\n",
    "\n",
    "        data = data[1:look_back] + [current_char]  # X data\n",
    "        target = 1 if before_char == '|' else 0  # y data\n",
    "\n",
    "        # look front\n",
    "        front_text = current_char\n",
    "        j = 0\n",
    "        while len(data) < look_back + look_front:\n",
    "            if i+j+1 < len(text):\n",
    "                if text[i+j+1] == '|':\n",
    "                    j += 1\n",
    "                    continue\n",
    "                data.append(text[i+j+1])\n",
    "                front_text += text[i+j+1]\n",
    "            else:\n",
    "                data.append('?')\n",
    "                front_text += '?'\n",
    "\n",
    "            j += 1\n",
    "        \n",
    "        ngram_vector = generate_dict_vector(front_text)\n",
    "        \n",
    "        str_data = '|'.join(data)\n",
    "        str_ngram_vector = '|'.join([str(x) for x in ngram_vector])\n",
    "        str_target = str(target)\n",
    "        # print(data, ngram_vector, target) \n",
    "\n",
    "        # print(str_data + '\\t' + str_ngram_vector + '\\t' + str_target)\n",
    "        with open(dataset_path, 'a') as f:\n",
    "            f.write(str_data + '\\t' + str_ngram_vector + '\\t' + str_target + '\\n')\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "    # y[-1] = 1  # end text is '|'\n",
    "    # return X, X_ngram, y\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def encode_data(X, y):\n",
    "    sequence_len = len(X[0])\n",
    "    encode_X = np.zeros((len(X), sequence_len, len(CHAR_INDICES)), dtype=np.float32)\n",
    "    encode_y = np.zeros((len(y), 2), dtype=np.float32)\n",
    "    for i, (data, target) in enumerate(zip(X, y)):\n",
    "        for t, char in enumerate(data):\n",
    "            encode_X[i, t, char] = 1\n",
    "        encode_y[i, target] = 1\n",
    "        \n",
    "    return encode_X, encode_y\n",
    "\n",
    "\n",
    "def read_text(file):\n",
    "    return open(file, encoding='utf-8', errors=\"ignore\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(text, wordlist):\n",
    "    for new_word, list_ in wordlist.items():\n",
    "        for old_word in list_:\n",
    "            text = text.replace(old_word.lower(), new_word)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocessing(text, wordlist):\n",
    "    text = text.lower()  # to lower-case\n",
    "    text = replace(text, wordlist)\n",
    "\n",
    "    # check if char is in CHAR_INDICES\n",
    "    for char in text:\n",
    "        if char not in CHAR_INDICES:\n",
    "            text = text.replace(char, '')\n",
    "            \n",
    "    return text\n",
    "\n",
    "wordlist = {\n",
    "    '': ['\\n'],\n",
    "}\n",
    "\n",
    "wordlist2 = copy.deepcopy(wordlist)\n",
    "wordlist2[''].append('|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../sastra/1463.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  86%|████████▌ | 870/1011 [00:03<00:00, 240.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../sastra/1403.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  83%|████████▎ | 17138/20557 [02:19<00:27, 123.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../sastra/1437.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  85%|████████▍ | 5884/6955 [01:11<00:12, 82.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../sastra/1538.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▎ | 70651/84463 [08:16<01:37, 142.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../sastra/1537.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  57%|█████▋    | 42242/74050 [04:55<03:52, 137.05it/s]"
     ]
    }
   ],
   "source": [
    "# TRAIN DATASET\n",
    "files = train_list\n",
    "look_back = 10\n",
    "look_front = 5\n",
    "copy_files = files.copy()\n",
    "random.shuffle(copy_files)\n",
    "\n",
    "for file in copy_files:\n",
    "    print(f'File: {file}')\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "    text = read_text(file)\n",
    "    text = preprocessing(text, wordlist)\n",
    "    create_dataset(text, look_back, look_front, file_name, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../sastra/1260.txt\n",
      "File: ../sastra/1536.txt\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION DATASET\n",
    "files = val_list\n",
    "look_back = 10\n",
    "look_front = 5\n",
    "copy_files = files.copy()\n",
    "random.shuffle(copy_files)\n",
    "\n",
    "for file in copy_files:\n",
    "    print(f'File: {file}')\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "    text = read_text(file)\n",
    "    text = preprocessing(text, wordlist)\n",
    "    create_dataset(text, look_back, look_front, file_name, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATASET\n",
    "files = test_list\n",
    "look_back = 10\n",
    "look_front = 5\n",
    "copy_files = files.copy()\n",
    "random.shuffle(copy_files)\n",
    "\n",
    "for file in copy_files:\n",
    "    print(f'File: {file}')\n",
    "    file_name = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "    text = read_text(file)\n",
    "    text = preprocessing(text, wordlist)\n",
    "    create_dataset(text, look_back, look_front, file_name, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
